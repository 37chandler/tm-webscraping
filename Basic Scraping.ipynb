{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook that does some basic webscraping using the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # To get the pages\n",
    "from bs4 import BeautifulSoup # and to process them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrape some webpages for some politicians. As I write this, the obvious candidates (pun intended) are Donald Trump and \n",
    "Joe Biden. Feel free to adjust the URLs to candidates that you find interesting. We may use these in some other contexts, so having two candidates on different sides of some issue could be nice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\"https://joebiden.com/\",\n",
    "         \"https://www.donaldjtrump.com/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the site in the first spot of our list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sites[0])\n",
    "r = requests.get(sites[0])\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you pull a page, it's a good idea to see what the status code is. Here's a [link](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) to what the numbers mean. \n",
    "\n",
    "Now let's look at the text that's on the page. Warning, this is going to be a mess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was right, that page was a mess, so let's try Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a prettier version, but it's not _that_ much prettier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool things we can do is search the soup to find things like `a` tags. Go look up what those tags are used for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a_tags = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the number of links on this page. Let's make a list of all of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_links = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    candidate_links.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we might want to do now is crawl each one of those pages to extract the text. Let's store the text in a dictionary that has the url as the key and the value is the text. One trick we'll use is to just extract visible text from the page, using the code found at this StackOverflow [answer](https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Comment\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_text = dict()\n",
    "\n",
    "for link in candidate_links :\n",
    "    try :\n",
    "        r = requests.get(link)\n",
    "    except :\n",
    "        pass \n",
    "    \n",
    "    if r.status_code == 200 :\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True)\n",
    "        visible_texts = filter(tag_visible, texts) \n",
    "        candidate_text[link] = \" \".join(t.strip() for t in visible_texts)\n",
    "    else :\n",
    "        print(f\"We got code {r.status_code} for this link: {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the results. Storing text data can be tricky, because often that text will have characters in it, like tabs and carriage returns, that we typically use to split up our files. We'll replace those with spaces in the file we're about to write out, so we can use tab delimiters. It's also nice to have a way to turn a URL into a nice file name. Here's a \n",
    "[function](https://stackoverflow.com/questions/9055249/simple-way-to-convert-a-url-into-a-filename)\n",
    "that does it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_url(url) :\n",
    "    \n",
    "    if not url :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https\n",
    "    name = url.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "\n",
    "    # Replace useless chareacters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # remove last underscore\n",
    "    last_underscore_spot = name.rfind(\"_\")\n",
    "    \n",
    "    name = name[:last_underscore_spot] + name[(last_underscore_spot+1):]\n",
    "\n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = generate_filename_from_url(sites[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_name,'w',encoding = \"UTF-8\") as outfile :\n",
    "    outfile.write(\"\\t\".join([\"link\",\"text\"]) + \"\\n\")\n",
    "    for link in candidate_text :\n",
    "        the_text = candidate_text[link]\n",
    "        \n",
    "        # get rid of some of our more annoying output chars\n",
    "        the_text = the_text.replace(\"\\t\",\" \").replace(\"\\n\",\" \").replace(\"\\r\",\" \") \n",
    "        \n",
    "        if not link :\n",
    "            link = \"empty link\"\n",
    "        \n",
    "        if the_text : # test to see if it is non-empty\n",
    "            outfile.write(\"\\t\".join([link,the_text]) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a new notebook with a name like \"Basic Scraping 2\". Rework this code so that it processes the full \n",
    "list of URLs in \"sites\", creating an output file for each site. Test it by adding a politician or two and scraping\n",
    "them all. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
